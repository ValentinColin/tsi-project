{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "689722e0-edfe-4baa-96c3-4bb4dfd2a4a5",
   "metadata": {},
   "source": [
    "# Preliminary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec92332-3776-4c77-989e-d9981104d68d",
   "metadata": {
    "tags": []
   },
   "source": [
    "\"\"\"\n",
    "import pyuac\n",
    "\n",
    "def main():\n",
    "    print(\"Do stuff here that requires being run as an admin.\")\n",
    "    # The window will disappear as soon as the program exits!\n",
    "    input(\"Press enter to close the window. >\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not pyuac.isUserAdmin():\n",
    "        print(\"Re-launching as admin!\")\n",
    "        pyuac.runAsAdmin()\n",
    "    else:        \n",
    "        main()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5c6c77c-9c8e-4c9e-8c86-c5ac7ca823c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d561f4c-bdf4-476e-bb1d-993844b996cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tasks = ['emotion', 'hate', 'irony', 'offensive', 'sentiment-latest']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034c8a11-6e76-44df-81f5-a59df883660d",
   "metadata": {},
   "source": [
    "# Process Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35625fc4-0bb7-4042-982a-898894b4e89b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocess text (username and link placeholders)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1aad9eda-2d3d-4778-9489-fda226fafb1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_labels(task):\n",
    "    # download label mapping\n",
    "    labels=[]\n",
    "    mapping_link = f\"./cardiffnlp/twitter-roberta-base-{task}/mapping.txt\"\n",
    "    with open(mapping_link, encoding=\"utf-8\") as f:\n",
    "        maps = f.read().split(\"\\n\")\n",
    "        csvreader = csv.reader(maps, delimiter='\\t')\n",
    "    labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65ea4c8f-dddc-4c83-b198-076a72b53049",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_analysis(task, text):\n",
    "    labels = get_labels(task)\n",
    "    MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "    \n",
    "    text = preprocess(text)\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    \n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "    print(f\"According to our {task} model, this tweet is :\")\n",
    "\n",
    "    for i in range(scores.shape[0]):\n",
    "        l = labels[ranking[i]]\n",
    "        s = scores[ranking[i]]\n",
    "        print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392b74fc-9b7c-463e-956c-bb24bc3b75f2",
   "metadata": {},
   "source": [
    "# Let's Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79e7ef36-745b-4cf7-a11d-1ce2d6c027f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to our emotion model, this tweet is :\n",
      "1) joy 0.9542\n",
      "2) optimism 0.0177\n",
      "3) anger 0.0145\n",
      "4) sadness 0.0136\n",
      "According to our hate model, this tweet is :\n",
      "1) not-hate 0.9523\n",
      "2) hate 0.0477\n",
      "According to our irony model, this tweet is :\n",
      "1) irony 0.9348\n",
      "2) non_irony 0.0652\n",
      "According to our offensive model, this tweet is :\n",
      "1) not-offensive 0.8711\n",
      "2) offensive 0.1289\n",
      "According to our sentiment-latest model, this tweet is :\n",
      "1) positive 0.9868\n",
      "2) neutral 0.009\n",
      "3) negative 0.0042\n"
     ]
    }
   ],
   "source": [
    "tweet = \"My IA teacher is so handsome !\"\n",
    "\n",
    "for task in tasks:\n",
    "    get_analysis(task, tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82cab25c-77ae-44ae-876d-93cee913cded",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prediction(task, text):\n",
    "    MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "    \n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    label = output[0][0].detach().numpy().argmax(axis = 0)\n",
    "    \n",
    "    return label\n",
    "\n",
    "#task = \"sentiment\"\n",
    "#print(get_labels(task))\n",
    "\n",
    "#label = prediction(\"sentiment\", \"My IA teacher is handsome\")\n",
    "#print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1901392b-af88-4e52-900f-dd8de988c152",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "pred_label = prediction(\"sentiment\", \"My IA teacher is handsome\")\n",
    "print(pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db1f0c7b-c625-4ebd-bf94-f707d41527f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from sklearn.metrics import confusion_matrix\n",
    "#import matplotlib.pyplot as plt\n",
    "#import matplotlib.font_manager as fm\n",
    "\n",
    "# Load the font file\n",
    "#font_path = \"./font/NotoEmoji-VariableFont_wght.ttf\"\n",
    "#font_prop = fm.FontProperties(fname=font_path)\n",
    "#plt.rcParams[\"font.family\"] = font_prop.get_name()\n",
    "#import os\n",
    "#print(os.path.isfile(font_path))z\n",
    "#if task == \"emoji\":\n",
    "#    font_path = \"font/NotoEmoji-Regular.ttf\"\n",
    "#    prop = font_manager.FontProperties(fname=font_path)\n",
    "#    legend_labels = ['read hearth', 'üòç', 'üòÇ', 'üíï', 'üî•', 'üòä', 'üòé', '‚ú®', 'blue hearth', 'üòò', 'üì∑', 'US flag', '‚òÄ', 'purple hearth', 'üòâ', 'üíØ', 'üòÅ', 'üéÑ', 'üì∏', 'üòú']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e83af925-ff9f-488c-bb66-e6bd314a375c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_confusion_matrix(conf_mat, list_labels, order_labels, task = \"\", data_set= \"\", nb_samples=10):\n",
    "    plt.imshow(conf_mat, cmap='YlOrRd')\n",
    "    #plt.plot(conf_mat, cmap='YlOrRd')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "\n",
    "    for i in range(len(conf_mat)):\n",
    "        for j in range(len(conf_mat[i])):\n",
    "            if i == j:\n",
    "                plt.text(j, i, int(round(conf_mat[i][j] * 100)), ha=\"center\", va=\"center\", color=\"white\")\n",
    "            else:\n",
    "                plt.text(j, i, int(round(conf_mat[i][j] * 100)), ha='center', va='center', color='black')\n",
    "\n",
    "    plt.xticks(order_labels)\n",
    "    plt.yticks(order_labels)\n",
    "    \n",
    "    plt.gca().set_xticklabels(list_labels, rotation=90)\n",
    "    plt.gca().set_yticklabels(list_labels)\n",
    "\n",
    "    #plt.title(\"+ de 25% du temps de travail a √©t√© √† dessiner ce graph, profitez-en\")\n",
    "    plt.title(f\"Confusion Matrix of the {task} model\")\n",
    "    plt.savefig(f\"error_analysis/confusion_matrix_{task}_{data_set}_{nb_samples}.png\",dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "638e7fbb-d252-49df-a001-fdc11bd44453",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def error_by_class_calculation(true_labels, pred_labels, order_labels):\n",
    "    conf_mat = confusion_matrix(true_labels, pred_labels, labels=order_labels)\n",
    "    wrong_class_prop = np.zeros((1, len(order_labels)))\n",
    "    sum_of_error = conf_mat.sum() - np.trace(conf_mat)\n",
    "    for i in range(len(conf_mat)):\n",
    "        wrong_class_prop[0, i] = (conf_mat[i, :].sum() - conf_mat[i, i]) / sum_of_error if sum_of_error != 0 else 0\n",
    "        \n",
    "    return wrong_class_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eaf6a93f-36cb-40ac-b244-708aabbf750a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def display_error_by_class(wrong_class_prop, list_labels, task = \"\", data_set= \"\", nb_samples=10):\n",
    "    (fig, ax) = plt.subplots(figsize=(7, 4))\n",
    "    plt.plot(range(len(list_labels)), wrong_class_prop[0], marker = 'o')\n",
    "    \n",
    "    y_axes = [(i/10) for i in range(6)]\n",
    "    ax.set_yticks(y_axes)\n",
    "\n",
    "    \n",
    "    plt.xticks(np.arange(len(list_labels)))\n",
    "    plt.gca().set_xticklabels(list_labels, rotation=90)\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('Portion of wrong label')\n",
    "    plt.title(\"Proportion of wrongly labeled text among the wrong labeled texts\")\n",
    "    plt.savefig(f\"error_analysis/error_by_class_{task}_{data_set}_{nb_samples}.png\",dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f905c03e-e8bf-4672-a41a-5e54ee9e1898",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def error_analysis(text_dataset, true_labels, task, data_set, nb_samples=10):\n",
    "    print(\"   data_set =\", data_set) \n",
    "    pred_labels = []\n",
    "    text_dataset = text_dataset[:nb_samples]\n",
    "    true_labels = true_labels[:nb_samples]\n",
    "    list_labels = get_labels(task)\n",
    "    order_labels = np.arange(len(list_labels))\n",
    "    \n",
    "    i = 0\n",
    "    for text in text_dataset:\n",
    "        i+=1\n",
    "        print(f\"text number {i}\")\n",
    "        pred_label = prediction(task, text)\n",
    "        pred_labels.append(pred_label)\n",
    "\n",
    "    pred_labels = np.array(pred_labels).flatten()\n",
    "    true_labels = np.array(true_labels).flatten()\n",
    "    \n",
    "    conf_mat = confusion_matrix(true_labels, pred_labels, labels=order_labels, normalize='true')\n",
    "    display_confusion_matrix(conf_mat, list_labels, order_labels, task, data_set, nb_samples)\n",
    "\n",
    "    wrong_class_prop = error_by_class_calculation(true_labels, pred_labels, order_labels)\n",
    "    display_error_by_class(wrong_class_prop, list_labels, task, data_set, nb_samples)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63a47b5d-8cce-460e-aae8-5503722a4d92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def error_analysis_specific_set(task, data_set, nb_sample=10):\n",
    "    df_data = pd.read_parquet(f\"./data_set/{task}/tweet_eval-{data_set}.parquet\")\n",
    "    \n",
    "    \n",
    "    texts = df_data['text'][:nb_sample].tolist()\n",
    "    labels = df_data['label'][:nb_sample].tolist()\n",
    "    \n",
    "    error_analysis(texts, labels, task, data_set, nb_samples = nb_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abe70fd9-bc3f-467f-9ece-c477ed64f460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task = emotion\n",
      "   data_set = test\n",
      "text number 1\n",
      "text number 2\n",
      "   data_set = train\n",
      "text number 1\n",
      "text number 2\n",
      "   data_set = validation\n",
      "text number 1\n",
      "text number 2\n",
      "task = hate\n",
      "   data_set = test\n",
      "text number 1\n",
      "text number 2\n",
      "   data_set = train\n",
      "text number 1\n",
      "text number 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask =\u001b[39m\u001b[38;5;124m\"\u001b[39m, task)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data_set \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m----> 5\u001b[0m     \u001b[43merror_analysis_specific_set\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m  \n",
      "Cell \u001b[1;32mIn[15], line 8\u001b[0m, in \u001b[0;36merror_analysis_specific_set\u001b[1;34m(task, data_set, nb_sample)\u001b[0m\n\u001b[0;32m      5\u001b[0m texts \u001b[38;5;241m=\u001b[39m df_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m][:nb_sample]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m      6\u001b[0m labels \u001b[38;5;241m=\u001b[39m df_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m][:nb_sample]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m----> 8\u001b[0m \u001b[43merror_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnb_sample\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 13\u001b[0m, in \u001b[0;36merror_analysis\u001b[1;34m(text_dataset, true_labels, task, data_set, nb_samples)\u001b[0m\n\u001b[0;32m     11\u001b[0m     i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext number \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m     pred_label \u001b[38;5;241m=\u001b[39m \u001b[43mprediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     pred_labels\u001b[38;5;241m.\u001b[39mappend(pred_label)\n\u001b[0;32m     16\u001b[0m pred_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(pred_labels)\u001b[38;5;241m.\u001b[39mflatten()\n",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m, in \u001b[0;36mprediction\u001b[1;34m(task, text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprediction\u001b[39m(task, text):\n\u001b[0;32m      2\u001b[0m     MODEL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcardiffnlp/twitter-roberta-base-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL)\n\u001b[0;32m      6\u001b[0m     encoded_input \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tsi-project-Fu6WFrE4-py3.8\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:658\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    655\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    656\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    657\u001b[0m         )\n\u001b[1;32m--> 658\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    660\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[0;32m    661\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tsi-project-Fu6WFrE4-py3.8\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1804\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1801\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1802\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1804\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1805\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1806\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1807\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1808\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1809\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1810\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1812\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1813\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1814\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tsi-project-Fu6WFrE4-py3.8\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1959\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, use_auth_token, cache_dir, local_files_only, _commit_hash, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1957\u001b[0m \u001b[38;5;66;03m# Instantiate tokenizer.\u001b[39;00m\n\u001b[0;32m   1958\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1959\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1960\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   1961\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m   1962\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1963\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1964\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tsi-project-Fu6WFrE4-py3.8\\lib\\site-packages\\transformers\\models\\roberta\\tokenization_roberta_fast.py:178\u001b[0m, in \u001b[0;36mRobertaTokenizerFast.__init__\u001b[1;34m(self, vocab_file, merges_file, tokenizer_file, errors, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, add_prefix_space, trim_offsets, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    163\u001b[0m     vocab_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    177\u001b[0m ):\n\u001b[1;32m--> 178\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmerges_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43msep_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcls_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcls_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43munk_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_prefix_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_prefix_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrim_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrim_offsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    195\u001b[0m     pre_tok_state \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend_tokenizer\u001b[38;5;241m.\u001b[39mpre_tokenizer\u001b[38;5;241m.\u001b[39m__getstate__())\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pre_tok_state\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd_prefix_space\u001b[39m\u001b[38;5;124m\"\u001b[39m, add_prefix_space) \u001b[38;5;241m!=\u001b[39m add_prefix_space:\n",
      "File \u001b[1;32m~\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\tsi-project-Fu6WFrE4-py3.8\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:128\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    121\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt instantiate the backend tokenizer from one of: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(1) a `tokenizers` library serialization file, \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    126\u001b[0m     )\n\u001b[1;32m--> 128\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m \u001b[38;5;241m=\u001b[39m fast_tokenizer\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    131\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(slow_tokenizer\u001b[38;5;241m.\u001b[39minit_kwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# √† faire tourner si on a le temps :)\n",
    "for task in tasks:\n",
    "    print(\"task =\", task)\n",
    "    for data_set in [\"test\", \"train\", \"validation\"]:\n",
    "        error_analysis_specific_set(task, data_set, nb_sample=100)  # √† faire quand on aura le temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529b9365-2895-4f69-90ef-8c2a44a11fe6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "error_analysis_specific_set(\"hate\", \"test\", nb_sample=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccfbe6f-c7ac-443a-9fd3-f720ce065d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_analysis_specific_set(\"sentiment-latest\", \"test\", nb_sample=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
